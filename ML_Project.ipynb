{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1f56278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus      import stopwords\n",
    "from nltk.stem        import WordNetLemmatizer\n",
    "from collections      import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    preprocess_string, \n",
    "    strip_tags, \n",
    "    strip_punctuation, \n",
    "    remove_stopwords, \n",
    "    strip_numeric, \n",
    "    strip_non_alphanum\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-chance",
   "metadata": {},
   "source": [
    "### Hyper Paramenters:\n",
    "    1) Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e524c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa609e1",
   "metadata": {},
   "source": [
    "## Class for Pre-Processing the News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2df6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessNews:\n",
    "    \"\"\"\n",
    "    A Class to clean the news text data\n",
    "    \"\"\"    \n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.filters = [\n",
    "            strip_tags,\n",
    "            strip_numeric,\n",
    "            strip_punctuation,\n",
    "            strip_non_alphanum,\n",
    "            lambda x: x.lower(),\n",
    "            remove_stopwords\n",
    "        ]\n",
    "        \n",
    "    def __call__(self, sentence):\n",
    "        processed_sentence = self.clean(sentence)\n",
    "        return processed_sentence\n",
    "    \n",
    "    def clean(self, sentence):\n",
    "        clean_words = [ \n",
    "            self.wnl.lemmatize(word, 'v') \n",
    "            for word in preprocess_string(sentence, self.filters) \n",
    "        ]\n",
    "        return \" \".join(clean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a68f02f",
   "metadata": {},
   "source": [
    "## Utililty Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9bbe6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tag_index(tags):\n",
    "    \"\"\"\n",
    "    A Fucntion that returns a 'tag to int' dictionary containig every unique tag\n",
    "    \"\"\"            \n",
    "    all_tags = {\n",
    "        t\n",
    "        for tag in tags\n",
    "        for t in tag.split(',')\n",
    "    }\n",
    "    return { tag: idx for idx, tag in enumerate(all_tags)  }\n",
    "\n",
    "def get_tags_vector(tags):\n",
    "    \"\"\"\n",
    "    A Function that converts a list of tags to a vector representation\n",
    "    \"\"\"    \n",
    "    tag_index = get_tag_index(all_tags)\n",
    "    tag_vector = np.zeros((len(tag_index),), dtype=int)\n",
    "    for tag in tags.split(','):\n",
    "        tag_vector[tag_index[tag]] = 1\n",
    "        \n",
    "    return tag_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b95213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(news):\n",
    "    \"\"\"\n",
    "    A Function that returns a set of all unique words in the news Text\n",
    "    \"\"\"    \n",
    "    all_words = set()\n",
    "    for each_news in news:\n",
    "        for word in each_news.split():\n",
    "            all_words.add(word)\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "def tokenize(news, vocab_to_int):\n",
    "    \"\"\"\n",
    "    A Function that converts words in text news into integers based on the vocab_to_int dictionary \n",
    "    \"\"\"    \n",
    "    tokenized_news = list()\n",
    "    for each_news in news:\n",
    "        temp = list()\n",
    "        for word in each_news.split():\n",
    "            temp.append(vocab_to_int[word])\n",
    "        tokenized_news.append(np.array(temp))\n",
    "        \n",
    "    return np.array(tokenized_news)\n",
    "\n",
    "def pad(news, required_length):\n",
    "    \"\"\"\n",
    "    A Function that makes all news examples of required_length, padding the smaller\n",
    "    and truncates the larger ones, from the end\n",
    "    \"\"\"    \n",
    "    padded_news = list()\n",
    "    for i, each_news in enumerate(news):\n",
    "        \n",
    "        each_news_len = len(each_news)\n",
    "        if (required_length >= each_news_len):\n",
    "            zeros   = np.zeros(required_length-each_news_len) \n",
    "            padded_news.append(np.concatenate([each_news, zeros]))\n",
    "        else:\n",
    "            padded_news.append(each_news[:required_length])\n",
    "            pass\n",
    "            \n",
    "    return np.array(padded_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "systematic-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(vocab_to_int, embedding_dict, dimension):\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(vocab_to_int)+1, dimension))\n",
    "\n",
    "    for word, index in vocab_to_int.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[index] = embedding_dict[word]\n",
    "        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab4c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    0: 'Barely True',\n",
    "    1: 'False '     ,\n",
    "    2: 'Half True'  ,\n",
    "    3: 'Mostly True',\n",
    "    4: 'True'       ,\n",
    "    5: 'Not Known'  ,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753f8fb",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "446dbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./archive/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9cd4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df     = pd.read_csv(path)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "labels   = df['Labels'].to_numpy()\n",
    "news     = df['Text'].apply(PreprocessNews()).to_numpy()\n",
    "all_tags = df['Text_Tag'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-microphone",
   "metadata": {},
   "source": [
    "## Tokenizing and Padding News Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words    = get_all_words(news)\n",
    "count_words  = Counter(all_words)\n",
    "total_words  = len(all_words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "\n",
    "#We are starting our vocab_to_int conversion dictionary from 1 because we will use '0' for padding\n",
    "vocab_to_int = { word:i+1 for i,(word, count) in enumerate(sorted_words) }\n",
    "\n",
    "tokenized_news = tokenize(news, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c2eefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1976, 6901, 7764, 5202,  941, 5398, 8150, 2844, 8453])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of Tokenized News\n",
    "tokenized_news[8080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acceptable-circumstances",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum News Length: 1\n",
      "Maximum News Length: 316\n",
      "Average News Length: 9.840398515335027\n",
      "Standard Deviation of Length: 5.633679829817566\n"
     ]
    }
   ],
   "source": [
    "#Getting some statistics from the news data to find a suitable sequence_length\n",
    "news_len = [ len(each_news) for each_news in tokenized_news ]\n",
    "\n",
    "min_len  = np.min (news_len)\n",
    "max_len  = np.max (news_len)\n",
    "avg_len  = np.mean(news_len)\n",
    "len_std  = np.std (news_len)\n",
    "print(\n",
    "        f\"Minimum News Length: {min_len}\\n\" +\n",
    "        f\"Maximum News Length: {max_len}\\n\" +\n",
    "        f\"Average News Length: {avg_len}\\n\" +\n",
    "        f\"Standard Deviation of Length: {len_std}\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "about-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Sequence Length: 15\n"
     ]
    }
   ],
   "source": [
    "sequence_length = int(avg_len+len_std)\n",
    "print(f\"Using Sequence Length: {sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907465d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_news = pad(tokenized_news, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce470c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1976., 6901., 7764., 5202.,  941., 5398., 8150., 2844., 8453.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of Padded Tokenized News\n",
    "padded_news[8080]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-passing",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Creating the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "invalid-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove           = pd.read_csv('./glove.6b/glove.6b.50d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove_embedding = { key: val.values for key, val in glove.T.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pediatric-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension = 50 \n",
    "embedding_matrix = create_embedding_matrix(vocab_to_int, glove_embedding, embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46358 , -1.0009  , -0.43789 , -0.65805 , -0.25003 , -0.16574 ,\n",
       "        0.68172 , -0.62092 ,  0.13461 ,  1.7284  ,  0.13523 , -0.2918  ,\n",
       "        0.78272 , -0.39106 , -0.52691 ,  0.3621  , -0.43799 , -0.30998 ,\n",
       "       -0.02848 , -0.065929,  0.54238 , -0.91495 , -0.053648,  0.18225 ,\n",
       "       -0.83547 , -0.78016 ,  0.36905 , -0.06426 ,  0.14134 , -0.31794 ,\n",
       "        1.8934  , -0.29255 ,  0.33738 , -1.2918  ,  0.59186 , -0.3477  ,\n",
       "        0.48815 , -0.37124 , -0.46158 , -0.37741 , -0.58314 , -0.64557 ,\n",
       "       -0.32752 ,  1.3523  ,  0.97032 , -0.87089 ,  0.89687 ,  0.6589  ,\n",
       "        0.68262 ,  0.65271 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of Embedding\n",
    "word = 'selective'\n",
    "embedding_matrix[vocab_to_int[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-easter",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wanted-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size  = embedding_matrix.shape[0]\n",
    "vector_size = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "renewable-insight",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = len(get_tag_index(all_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "favorite-anthony",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNetwork(nn.Module):\n",
    "    \n",
    "    #constructor\n",
    "    def __init__(self, hidden_1, hidden_2, hidden_3, num_layers, bidirectional, output_dim, p):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        #Embedding Layer\n",
    "        self.embedding        = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vector_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        #Recurrent Layers\n",
    "        self.lstm = nn.LSTM(\n",
    "                            input_size   =embedding_dimension, \n",
    "                            hidden_size  =hidden_1,\n",
    "                            num_layers   =num_layers, \n",
    "                            bidirectional=False,\n",
    "                            batch_first  =True,\n",
    "                            dropout      =p\n",
    "                           )\n",
    "        \n",
    "        #Fully Connected Layers\n",
    "        self.linear1 = nn.Linear(hidden_1+num_tags, hidden_2)\n",
    "        self.linear2 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.linear2 = nn.Linear(hidden_3, output_dim)\n",
    "        self.relu    = nn.ReLU()\n",
    "        \n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, text, tags):\n",
    "        embeddings = self.embeddings(text)\n",
    "        x1         = self.lstm()(embeddings)\n",
    "        print(type)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rotary-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding   = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "direct-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_news[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "virtual-diary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 50])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.LongTensor(padded_news[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "happy-breach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10238"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "commercial-liquid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8190, 15)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_ratio = 0.8\n",
    "\n",
    "train_news      = padded_news[:int(len(padded_news)*split_ratio)] \n",
    "validation_news = padded_news[int(len(padded_news)*split_ratio):]\n",
    "train_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "prescribed-genesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 50])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = train_news[:32]\n",
    "batch1 = embedding(torch.LongTensor(batch1))\n",
    "batch1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "greater-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "#Creating the DataLoadres\n",
    "train_loader      = DataLoader(dataset=train_news, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(dataset=validation_news, batch_size=batch_size,   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "raised-halloween",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<torch.utils.data.dataloader.DataLoader object at 0x000002286330CA60>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(train_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cardiovascular-robert",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got DataLoader)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-614eb23e9546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: new(): data must be a sequence (got DataLoader)"
     ]
    }
   ],
   "source": [
    "embedding(torch.LongTensor(train_loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "distinct-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(type(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-country",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
